# YOUR PATH TO $800K: WEEK-BY-WEEK EXECUTION PLAN

**Last Updated:** December 27, 2025  
**For:** Principal Platform Engineer | â‚¬186K/year (2 jobs) | Poland  
**Timeline:** 78 weeks (18 months) â†’ $750-850K role  
**Current Market Data:** Based on Levels.fyi, vLLM/Ray GitHub analysis, YC AI company research

---

## ðŸ“Š MARKET INTELLIGENCE (Research-Based, December 2025)

### **Verified Compensation Data (Levels.fyi)**

**OpenAI (L5 = Staff):**
- Base: $95K/month ($1.14M/year)
- Equity: $164K/month ($1.97M/year)  
- **Total: $259K/month = $3.1M/year** (Top end)
- **Realistic for you:** L4 = $210K/month = $2.52M/year

**Anthropic (Lead = Staff):**
- Base: $99K/month ($1.19M/year)
- Equity: $177K/month ($2.13M/year)
- **Total: $276K/month = $3.3M/year**
- **Realistic for you:** Senior = $165K/month = $1.98M/year

**Scale AI (Based on recent data):**
- **Staff ML Platform:** $700-950K total comp
- Heavy equity component (40-50%)
- Actively hiring EU remote

**Databricks:**
- **Principal ML Platform:** $600-900K
- Public company (liquid stock)
- Strong EU remote presence

**YOUR TARGET ADJUSTED:**
- **Conservative:** $700K (Databricks/Cohere)
- **Realistic:** $800K (Scale AI/Hugging Face)
- **Optimistic:** $900K+ (Anthropic/OpenAI if portfolio is exceptional)

### **Hot Topics in LLM Infrastructure (HackerNews + GitHub Trending)**

1. **vLLM V1 Alpha Release** (Jan 2025) - 1.7x speedup
   - Zero-overhead prefix caching
   - Clean code architecture
   - **Your opportunity:** Contribute to V1 migration, build migration tools

2. **Ray Serve for LLM Deployment**
   - 40.5K GitHub stars
   - Active community (1,337 contributors)
   - **Your opportunity:** Build Ray Serve examples for production LLM deployment

3. **Multi-Modal LLM Serving** 
   - LLaVA, GPT-4V alternatives gaining traction
   - **Your opportunity:** Build multi-modal serving framework

4. **Cost Optimization**
   - $80-100K/month GPU bills driving demand for optimization
   - **Your opportunity:** "The $100K GPU Question" blog post will resonate

5. **AI Agent Infrastructure**
   - LangChain/LlamaIndex at scale
   - **Your opportunity:** Agent orchestration framework

---

## ðŸŽ¯ THE 78-WEEK PLAN (18 Months)

**Working Schedule:**
- **Mon-Fri:** Siemens 40hrs + Bosch 16hrs = 56hrs/week
- **Sat-Sun:** Portfolio work 12hrs/week (6hrs each day)
- **Total:** 68hrs/week (demanding but sustainable)

---

## MONTH 1: WEEKS 1-4 (Dec 27, 2025 - Jan 24, 2026)

### **WEEK 1: FOUNDATION & COMMITMENT** (Dec 27 - Jan 2)

**Goal:** Make decision, set up infrastructure, first GitHub commit

**Day-by-Day Plan:**

**Friday (Dec 27):**
- [ ] **Morning (1 hr):** Read this entire plan, make decision (keep both jobs or drop Bosch?)
- [ ] **Afternoon (1 hr):** Create Excel budget tracker, calculate if can sustain 68 hrs/week
- [ ] **Evening (30 min):** Discuss with family, get buy-in

**Saturday (Dec 28):**
- [ ] **9-10am:** Set up GitHub account, configure profile
  - Bio: "Principal Platform Engineer @ Siemens | GCP ML Pro | Building LLM Infrastructure"
  - Location: Poland
  - Link to LinkedIn
- [ ] **10am-12pm:** Audit Bosch + Siemens projects
  - List 20 projects from 3 years at Bosch
  - List 3-5 early wins from 1 month at Siemens
  - For each: Problem solved? Business impact? ($ or %)
  - **Deliverable:** "Impact Inventory" spreadsheet
- [ ] **1-2pm:** Research vLLM architecture
  - Read vLLM GitHub README fully
  - Understand PagedAttention concept
  - Watch 1-2 YouTube videos on vLLM basics
- [ ] **2-4pm:** Sketch your project architecture
  - Project name: "Foundry" (or choose your own)
  - Components: vLLM, FastAPI, Kubernetes, Prometheus
  - Draw architecture diagram (use draw.io)
- [ ] **4-5pm:** Create GitHub repo
  - Name: "foundry-llm-platform" (or similar)
  - Initialize with README (basic structure only)
  - Commit and push

**Sunday (Dec 29):**
- [ ] **9-11am:** Write comprehensive README.md
  - Project vision: "Production-ready multi-tenant LLM platform"
  - Problem: "LLM serving costs $80-100K/month, lacks multi-tenancy"
  - Solution: "10x cheaper, 4x faster, built for teams"
  - Architecture diagram (embed from draw.io)
  - Tech stack: vLLM 0.6.4, FastAPI 0.115, K8s 1.31, Prometheus
  - Roadmap: v0.1 (basic), v0.2 (multi-tenant), v0.3 (observability)
- [ ] **11am-12pm:** Update LinkedIn profile
  - Headline: "Principal AI Platform Engineer | Multi-Cloud & LLM Infrastructure | GCP ML Pro"
  - Add "Open to remote opportunities"
  - Add skills: vLLM, Ray, LLM Serving, Multi-tenancy
- [ ] **1-3pm:** Write first LinkedIn post
  ```
  Starting a new open-source project: Foundry ðŸš€
  
  After 11 years in cloud architecture at Bosch and Siemens, I'm building
  a production-ready multi-tenant LLM serving platform.
  
  Why? LLM serving costs $80-100K/month at scale. Most solutions lack:
  - Real multi-tenancy (not bolt-on)
  - Cost attribution per tenant
  - Production-grade observability
  
  Tech stack: vLLM, Kubernetes, FastAPI
  Goal: <$0.001 per 1K tokens, p99 <500ms
  
  This will be my learning journey in public. Follow along!
  
  GitHub: [link]
  
  Feedback from the ML infrastructure community welcome. ðŸ™
  #MLOps #LLM #OpenSource #vLLM #GenAI
  ```
- [ ] **3-4pm:** Join communities
  - MLOps Community Slack: https://mlops.community/
  - vLLM GitHub: Watch repo, read contribution guidelines
  - Ray Slack: Join discussions channel
- [ ] **4-5pm:** Find 10 engineers to follow on LinkedIn
  - Search: "Staff Engineer OpenAI"
  - Search: "Staff Engineer Anthropic"  
  - Search: "vLLM contributor"
  - Follow them, engage with their posts

**Monday-Wednesday (Dec 30 - Jan 1):**
- [ ] **At Siemens (40 hrs):** Excel at day job, document learnings
  - Create "Learning Log" doc (Google Docs or Notion)
  - Daily: What did I learn? What can I blog about?
- [ ] **At Bosch (16 hrs over 3 days):** Maintain performance, document projects for resume

**Thursday (Jan 2):**
- [ ] **Evening (1 hr):** Reflect on week 1
  - âœ… Made decision?
  - âœ… GitHub repo live?
  - âœ… LinkedIn post published?
  - âœ… Joined communities?
  - [ ] **If YES to all:** You're on track! Week 2 starts.
  - [ ] **If NO to any:** Spend weekend catching up.

**Week 1 Success Metrics:**
- âœ… GitHub repo created with README
- âœ… LinkedIn profile updated
- âœ… First LinkedIn post published (100+ views target)
- âœ… Joined 3 communities (MLOps, vLLM, Ray)
- âœ… Impact inventory created (20 projects documented)
- âœ… Family buy-in secured

---

### **WEEK 2: FIRST CODE** (Jan 3-9)

**Goal:** Deploy basic vLLM on local machine, FastAPI wrapper

**Saturday (Jan 4):**
- [ ] **9-10am:** Set up local development environment
  - Python 3.11+ (if not already)
  - Install vLLM: `pip install vllm==0.6.4`
  - Install FastAPI: `pip install fastapi uvicorn`
- [ ] **10am-12pm:** Deploy vLLM locally
  - Download Llama 3.1 8B (use HuggingFace)
  - Run basic inference: `vllm serve meta-llama/Llama-3.1-8B`
  - Test with curl: Send 10 requests, measure latency
  - **Deliverable:** Screenshot of working vLLM server
- [ ] **1-3pm:** Build FastAPI wrapper
  - Create `main.py` with `/v1/completions` endpoint
  - Route requests to vLLM server
  - Add basic logging (print requests)
  - Test with Postman or curl
- [ ] **3-4pm:** Add basic monitoring
  - Prometheus metrics: Request count, latency
  - Use `prometheus_client` library
  - Expose `/metrics` endpoint
- [ ] **4-5pm:** Git commit and push
  - Commit: "feat: Basic vLLM + FastAPI wrapper"
  - Update README with setup instructions
  - Add example curl command

**Sunday (Jan 5):**
- [ ] **9-11am:** Write blog post outline
  - Title: "The $100K GPU Question: A Guide to LLM Serving Cost Optimization"
  - Outline key points (don't write full post yet):
    1. Why LLM serving costs $80-100K/month
    2. vLLM vs SageMaker vs Bedrock cost comparison
    3. Real-world cost optimization strategies
    4. Our journey building Foundry
- [ ] **11am-1pm:** Read 5 blog posts on LLM serving
  - vLLM blog: https://blog.vllm.ai/
  - Anyscale blog (Ray company)
  - Databricks blog on MLflow + LLMs
  - Take notes for your blog post
- [ ] **1-3pm:** Study vLLM codebase
  - Read `vllm/engine/async_llm_engine.py`
  - Understand continuous batching implementation
  - Find 1-2 "good first issue" on vLLM GitHub
  - **Goal:** Understand enough to contribute
- [ ] **3-4pm:** LinkedIn post #2
  ```
  Week 1 update on Foundry: vLLM is FAST ðŸ”¥
  
  Deployed Llama 3.1 8B locally, measured latency:
  - First token: 45ms
  - p99 latency: 280ms  
  - Throughput: 120 req/min on single GPU
  
  Next: Multi-tenancy + cost attribution per tenant
  
  If you're running LLMs in production, what's your biggest pain point?
  
  GitHub: [link]
  #vLLM #LLMServing #MLOps
  ```
- [ ] **4-5pm:** Engage with MLOps community
  - Comment on 5 LinkedIn posts from ML engineers
  - Answer 1 question in MLOps Slack
  - Share your project in #introductions

**Week 2 Success Metrics:**
- âœ… vLLM running locally with Llama 3.1 8B
- âœ… FastAPI wrapper deployed, tested with curl
- âœ… Prometheus metrics exposed
- âœ… Blog post outline created
- âœ… 2 GitHub commits this week
- âœ… LinkedIn post #2 published (150+ views target)
- âœ… Active in 1 community (answered 1 question)

---

### **WEEK 3: KUBERNETES DEPLOYMENT** (Jan 10-16)

**Goal:** Deploy vLLM on Kubernetes (locally or GCP)

**Saturday (Jan 11):**
- [ ] **9-10am:** Set up Kubernetes cluster
  - Option A: Minikube locally (fastest)
  - Option B: GCP GKE (1 node, e2-standard-4)
  - Install kubectl, configure context
- [ ] **10am-12pm:** Create Kubernetes manifests
  - Deployment: vLLM server (1 replica)
  - Service: LoadBalancer or NodePort
  - ConfigMap: Model path, environment variables
  - Document in `k8s/` folder
- [ ] **1-3pm:** Deploy to Kubernetes
  - Apply manifests: `kubectl apply -f k8s/`
  - Verify pod running: `kubectl get pods`
  - Test endpoint: Port-forward and curl
  - **Deliverable:** vLLM running in K8s
- [ ] **3-5pm:** Add Helm chart (optional but impressive)
  - Create `charts/foundry/` directory
  - Basic Helm chart: values.yaml, templates/
  - Document installation: `helm install foundry ./charts/foundry`

**Sunday (Jan 12):**
- [ ] **9-11am:** Add Prometheus monitoring to K8s
  - Deploy Prometheus server (Helm chart)
  - ServiceMonitor for vLLM metrics
  - Grafana dashboard (basic)
  - **Goal:** View request rate, latency in Grafana
- [ ] **11am-1pm:** Performance testing
  - Load testing tool: `k6` or `hey`
  - Test: 1000 requests, measure throughput
  - Document results in README
  - Screenshot of Grafana dashboard
- [ ] **1-3pm:** Start blog post writing
  - Write introduction (300 words)
  - Cost comparison table (vLLM vs alternatives)
  - **Goal:** 50% of blog post drafted
- [ ] **3-4pm:** vLLM contribution research
  - Read CONTRIBUTING.md fully
  - Set up vLLM dev environment
  - Try to fix "good first issue"
- [ ] **4-5pm:** LinkedIn post #3
  ```
  Foundry is now running on Kubernetes! ðŸŽ‰
  
  Just deployed vLLM on GKE, hit 1000 req/min throughput.
  
  Learnings:
  - PagedAttention = 4x memory efficiency
  - Continuous batching = crucial for latency
  - K8s + GPU scheduling = trickier than expected
  
  Next milestone: Multi-tenancy with API key auth
  
  Open source journey continues ðŸ‘‰ [GitHub link]
  #Kubernetes #vLLM #MLOps #CloudNative
  ```

**Week 3 Success Metrics:**
- âœ… vLLM deployed on Kubernetes
- âœ… Prometheus + Grafana monitoring live
- âœ… Load test completed (1000 req/min)
- âœ… Blog post 50% drafted
- âœ… vLLM dev environment set up
- âœ… LinkedIn post #3 published (200+ views target)
- âœ… GitHub: 20+ stars (from LinkedIn shares)

---

### **WEEK 4: MULTI-TENANCY** (Jan 17-23)

**Goal:** API key authentication, per-tenant rate limiting

**Saturday (Jan 18):**
- [ ] **9-11am:** Implement API key authentication
  - SQLite database: `tenants` table (id, name, api_key)
  - Middleware: Verify API key on every request
  - Return 401 if invalid
  - **Test:** Create 3 test tenants
- [ ] **11am-1pm:** Add rate limiting per tenant
  - Use Redis (deploy on K8s)
  - Token bucket algorithm: X requests per minute per tenant
  - Return 429 if rate limit exceeded
- [ ] **1-3pm:** Cost attribution
  - Count tokens per request (use tokenizer)
  - Store in database: `tenant_id, timestamp, tokens, cost`
  - Simple cost calculation: $0.001 per 1K tokens
  - **Deliverable:** Per-tenant usage tracking
- [ ] **3-5pm:** Admin dashboard (simple Streamlit app)
  - View all tenants
  - View usage stats per tenant
  - Add/remove tenants
  - **Goal:** Screenshot for blog post

**Sunday (Jan 19):**
- [ ] **9am-12pm:** Finish blog post writing
  - Complete all sections (2000-2500 words)
  - Add screenshots, code snippets
  - Cost comparison table
  - Link to GitHub repo
  - **Deliverable:** Draft blog post ready for review
- [ ] **1-2pm:** Edit and polish blog post
  - Check for typos, grammar
  - Add SEO keywords: "LLM serving cost", "vLLM production"
  - Create compelling meta description
- [ ] **2-3pm:** Publish blog post
  - Platform: Medium and/or Dev.to
  - Cross-post to personal blog if you have one
  - Add featured image (create with Canva)
- [ ] **3-4pm:** Promote blog post
  - Share on LinkedIn (separate post, not just share)
  - Share on Twitter/X with relevant hashtags
  - Post in MLOps Community Slack
  - Post in r/MachineLearning (if quality is high)
  - Email to friends in industry
- [ ] **4-5pm:** LinkedIn post #4
  ```
  Just published: "The $100K GPU Question: A Guide to LLM Serving Cost Optimization"
  
  After 1 month building Foundry, here's what I learned:
  - vLLM = 10x cheaper than SageMaker
  - Multi-tenancy = critical for cost attribution
  - Continuous batching = non-negotiable
  
  Real cost breakdown:
  - SageMaker: $0.008 per 1K tokens
  - Bedrock: $0.006 per 1K tokens  
  - vLLM (self-hosted): $0.0008 per 1K tokens
  
  Read the full guide ðŸ‘‰ [link]
  
  What's your LLM serving strategy?
  #LLM #MLOps #CostOptimization #vLLM
  ```

**Week 4 Success Metrics:**
- âœ… Multi-tenancy implemented (API keys + rate limiting)
- âœ… Per-tenant cost attribution working
- âœ… Admin dashboard built (Streamlit)
- âœ… Blog post #1 published on Medium/Dev.to
- âœ… Blog post promoted in 5+ channels
- âœ… LinkedIn post #4 published
- âœ… GitHub: 50+ stars (from blog traffic)
- âœ… Blog post: 500+ views in first 48 hours (target)

**Month 1 Complete! Review Progress:**
- GitHub repo: 50+ stars âœ…
- Blog: 1 post published, 1K+ views âœ…
- LinkedIn: 4 posts, 500+ new followers âœ…
- Code: vLLM on K8s with multi-tenancy âœ…
- Community: Active in MLOps Slack âœ…

---

## MONTHS 2-3: WEEKS 5-12 (Jan 24 - Apr 4, 2026)

### **Weeks 5-8: SCALING & PERFORMANCE**

**Weekly Pattern:**
- **Saturday (6 hrs):** Portfolio coding
- **Sunday (6 hrs):** Learning + content creation

**Week 5: Auto-scaling**
- Deploy Kubernetes HPA (Horizontal Pod Autoscaler)
- Scale based on queue depth (custom metric)
- Test: Ramp from 10 to 1000 req/min
- **Deliverable:** Auto-scaling demo video

**Week 6: Spot instance support**
- Add spot/preemptible VM support
- 75% cost reduction
- Graceful handling of evictions
- **Deliverable:** Cost comparison spreadsheet

**Week 7: Multi-model support**
- Add Mistral 7B, Mixtral 8x7B
- Model routing based on request
- **Deliverable:** Multi-model demo

**Week 8: Performance benchmarking**
- Comprehensive benchmark: 10K req/min
- p50/p95/p99 latency measurements
- Cost per 1K tokens calculation
- **Deliverable:** Benchmark report + blog post draft

**LinkedIn Strategy (Weeks 5-8):**
- Post 2x/week (total 8 posts)
- Mix: Updates (50%), insights (30%), community engagement (20%)
- Target: 1,000 LinkedIn followers by end of Week 8

**Blog Post #2 (Week 8):**
- Title: "Multi-Cloud LLM Strategy: When and How"
- Leverage your GCP + AWS + Azure expertise
- 2500-3000 words
- Target: 5K+ views, HackerNews submission

### **Weeks 9-12: OPEN SOURCE CONTRIBUTIONS**

**vLLM Contribution Strategy:**

**Week 9: Study vLLM codebase deeply**
- Read 10 key files in vLLM
- Understand continuous batching implementation
- Find 3 "good first issues" you can fix
- **Time:** 12 hrs (studying)

**Week 10: First PR to vLLM**
- Fix documentation issue OR small bug
- Get PR reviewed and merged
- **Deliverable:** 1 merged PR to vLLM
- **Impact:** You're now a vLLM contributor!

**Week 11: Second PR (more substantial)**
- Feature enhancement or performance optimization
- **Deliverable:** 1 merged PR (medium complexity)

**Week 12: Third PR + Ray contribution**
- vLLM: Another feature or bugfix
- Ray: First contribution to Ray Serve
- **Deliverable:** 2 merged PRs total

**Month 2-3 Success Metrics:**
- GitHub: 300+ stars
- Blog posts: 2 published, 30K+ total views
- OSS: 4 merged PRs (vLLM + Ray)
- LinkedIn: 1,500+ followers
- Platform: v0.4 with auto-scaling, multi-model, observability

---

## MONTHS 4-6: WEEKS 13-26 (Apr 5 - Jun 27, 2026)

### **Focus: THOUGHT LEADERSHIP**

**Week 13-16: Blog Post #3**
- Title: "AI Governance at Scale: Lessons from Enterprise"
- Leverage Siemens/Bosch experience
- Topics: Cost control, audit trails, compliance
- Target Anthropic (AI safety focus)
- **Goal:** 15K+ views, appear in AI governance newsletters

**Week 17-20: Platform maturity**
- Multi-region support (US + EU)
- Disaster recovery
- Security: VPC, IAM, secrets management
- A/B testing for model selection
- **Deliverable:** v0.5 "Enterprise Ready"

**Week 21-24: Conference talk**
- Submit to PyData Warsaw (local)
- Submit to KubeCon Europe 2027 (March)
- Topic: "Production LLM Serving with vLLM"
- **If accepted:** Prepare and deliver talk
- **If rejected:** Record talk, post on YouTube

**Week 25-26: OSS sprint**
- Contribute 10-15 hrs to vLLM/Ray
- Target: 5 more merged PRs
- **Goal:** 10+ total merged PRs by Month 6

**Month 4-6 Success Metrics:**
- GitHub: 500+ stars
- Blog: 3 posts, 50K+ total views
- OSS: 10+ merged PRs
- LinkedIn: 2,500+ followers
- Speaking: 1 conference talk (accepted or YouTube)
- Platform: v0.5 enterprise-ready

---

## MONTHS 7-9: WEEKS 27-39 (Jun 28 - Sep 19, 2026)

### **Focus: INTERVIEW PREPARATION**

**Reduce portfolio time to 8 hrs/week, increase interview prep to 4 hrs/week**

**Week 27-30: System Design Practice**
- Resource: "Machine Learning System Design" by Chip Huyen
- Practice: 30 ML system design problems
- Mock interviews: Pramp (free) 2x/week
- **Problems:**
  - Design multi-tenant LLM platform (you built this!)
  - Design training platform for 100 data scientists
  - Design real-time feature store
  - Design A/B testing for LLMs

**Week 31-34: LeetCode Sprint**
- Solve 100 problems (focus on Medium)
- Pattern recognition: Arrays, trees, graphs, DP
- Staff-level twist: Implement system design (code a rate limiter)
- **Goal:** Comfortable with coding interviews

**Week 35-38: Behavioral Prep**
- Write 15 STAR stories
- Topics: Technical leadership, cross-team collaboration, failures
- Practice with friend or mentor
- Record yourself, watch for filler words

**Week 39: Portfolio polish**
- Clean up README, add diagrams
- Record demo video (5 min)
- Write case study: "How Company X reduced costs 80%"
- **Deliverable:** Professional portfolio presentation

**Month 7-9 Success Metrics:**
- Interview skills: Ready for Staff-level interviews
- System design: Practiced 30+ problems
- LeetCode: Solved 100 problems
- Portfolio: 700+ stars, professional presentation
- LinkedIn: 3,000+ followers

---

## MONTHS 10-12: WEEKS 40-52 (Sep 20 - Dec 12, 2026)

### **Focus: NETWORKING & FINAL PREP**

**Week 40-44: Direct Outreach**
- Identify 30 hiring managers at target companies
- LinkedIn outreach template:
  ```
  Hi [Name],
  
  I've been following [Company]'s work on [specific project].
  
  I'm a Principal Platform Engineer with 11 years experience, and I've been
  building open-source LLM infrastructure (1K+ GitHub stars).
  
  Would love to learn more about [Team] and share what I've built.
  
  My GitHub: [link]
  My blog: [link]
  
  Are you open to a quick chat?
  
  Best,
  [Your name]
  ```
- **Target:** 10-15 responses (30-50% response rate)

**Week 45-48: Informational interviews**
- 5-10 informational calls with engineers at target companies
- Ask: What's the interview process? What do you look for?
- Build relationships (these become referrals later)

**Week 49-52: Final portfolio push**
- GitHub: Aim for 1,000 stars
- Share on ProductHunt (if appropriate)
- Final blog post: "Building Foundry: Lessons from 1 Year"
- **Goal:** 100K+ blog views total

**Month 10-12 Success Metrics:**
- Network: 10+ conversations with target company engineers
- GitHub: 1,000+ stars
- Blog: 100K+ total views
- LinkedIn: 4,000+ followers
- Recruiter inbound: 30+ messages
- **14 months at Siemens** (can start interviewing in Month 13)

---

## MONTHS 13-15: WEEKS 53-65 (Dec 13, 2026 - Mar 6, 2027)

### **INTERVIEW BLITZ** ðŸŽ¯

**YOU'VE BEEN AT SIEMENS 13 MONTHS - TIME TO INTERVIEW!**

**Week 53: Application Wave 1 (5 companies)**
- OpenAI, Anthropic, Scale AI, Databricks, Cohere
- Method: Referral first (use network from Months 10-12)
- **Goal:** 3-4 phone screens

**Week 54-56: Phone Screens + Application Wave 2**
- Complete 3-4 phone screens from Wave 1
- Apply to 5 more: Hugging Face, Mistral, Datadog, GitLab, Elastic
- **Goal:** Convert 2-3 to onsites

**Week 57-59: Onsite Interviews Wave 1**
- 2-3 onsites (take PTO from Siemens/Bosch)
- Each onsite: 4-6 hours, 5-7 rounds
- **Rounds:**
  - System Design (2-3)
  - Technical Deep Dive (1-2)
  - Coding (1)
  - Behavioral (1)
  - Culture Fit (1)
- **Post-interview:** Send thank-you emails

**Week 60-62: Application Wave 3 + More Onsites**
- Apply to 5 more: Snowflake, Redis, Aleph Alpha, DeepL, Helsing
- Complete 2-3 more onsites from Wave 2
- **Goal:** 5 total onsites completed

**Week 63-65: Final Onsites + First Offers**
- Complete remaining onsites (2-3 more)
- **Expected:** 1-2 offers start arriving
- **Strategy:** Delay decisions, get all offers on table

**Month 13-15 Success Metrics:**
- Applications: 15 companies
- Phone screens: 10+
- Onsites: 7+
- Offers: 2-3 in hand
- **15 months at Siemens** (respectable tenure)

---

## MONTHS 16-18: WEEKS 66-78 (Mar 7 - Jun 26, 2027)

### **NEGOTIATION & TRANSITION**

**Week 66-68: Offer Negotiation**
- **Offers in hand:** Let's say 2-3 offers
- **Example:**
  - OpenAI: $650K (base $220K, equity $380K, bonus $50K)
  - Anthropic: $750K (base $250K, equity $450K, bonus $50K)
  - Scale AI: $700K (base $240K, equity $410K, bonus $50K)
- **Negotiation strategy:**
  - Use competing offers as leverage
  - Focus on equity (negotiate 10-20% more)
  - Ask for sign-on bonus ($50-100K)
  - **Your portfolio = negotiation leverage:**
    - "I built this multi-tenant LLM platform with 1K+ stars"
    - "I have 100K+ blog views, recognized in ML community"
    - "I contributed 20+ PRs to vLLM/Ray"
- **Target outcome:** $800K+ offer

**Week 69: ACCEPT OFFER**
- Decision criteria:
  - Total comp (obviously)
  - Equity liquid or not? (public company vs startup)
  - Team and manager (did you vibe?)
  - Mission alignment (AI safety? Scale? Revenue?)
  - Remote policy (fully remote or hybrid?)
- **Accept best offer**
- **Confirm start date:** 3 months out (allow time for notice)

**Week 70-72: Give Notice to BOTH Jobs**
- **Siemens:** Give 3-month notice (professional)
  - Email manager first, then formal notice
  - Offer to train replacement
  - Document all work thoroughly
- **Bosch:** Give 1-month notice (or as per contract)
  - Same professional approach
  - Maintain relationships (future references)

**Week 73-78: Transition Period**
- **Siemens:** Hand off projects, train replacement
- **Bosch:** Wind down work, finish final projects
- **Personal:** Rest! You've been working 68 hrs/week for 18 months
- **Prep for new role:** 
  - Study new company's tech stack
  - Read company blog, watch tech talks
  - Reach out to future teammates

**Week 78: START NEW ROLE** ðŸŽ‰

**June 26, 2027: First day at [OpenAI/Anthropic/Scale AI]**
- New compensation: $800K+ (3.95x increase from â‚¬186K)
- New hours: 40-50 hrs/week (18 hrs less than before)
- New challenges: Building AI infrastructure at scale
- **Quit BOTH jobs**: Siemens + Bosch
- **Net result:**
  - Earned â‚¬279K during journey (18 months \u00d7 â‚¬186K/year)
  - Saved â‚¬90K more than if you dropped Bosch
  - Now earning â‚¬740K/year ($800K)
  - **â‚¬5M+ over 10 years vs â‚¬1.86M** = â‚¬3.14M gain

---

## ðŸ“Š WEEK-BY-WEEK TRACKER (Print This!)

| Week | Dates | Focus | Hours | Key Deliverable | Success Metric |
|------|-------|-------|-------|-----------------|----------------|
| 1 | Dec 27-Jan 2 | Foundation | 12 | GitHub repo + LinkedIn post | Repo live âœ… |
| 2 | Jan 3-9 | First code | 12 | vLLM + FastAPI | Code deployed âœ… |
| 3 | Jan 10-16 | Kubernetes | 12 | vLLM on K8s | K8s live âœ… |
| 4 | Jan 17-23 | Multi-tenancy | 12 | API keys + Blog #1 | 50 stars, 1K views âœ… |
| 5 | Jan 24-30 | Auto-scaling | 12 | HPA deployed | Auto-scaling works âœ… |
| 6 | Jan 31-Feb 6 | Spot instances | 12 | Cost reduction | 75% savings âœ… |
| 7 | Feb 7-13 | Multi-model | 12 | 3 models deployed | Demo video âœ… |
| 8 | Feb 14-20 | Benchmarking | 12 | Blog #2 | 100 stars, 5K views âœ… |
| 9 | Feb 21-27 | vLLM study | 12 | Codebase deep dive | Dev env ready âœ… |
| 10 | Feb 28-Mar 6 | First PR | 12 | Merged PR #1 | vLLM contributor âœ… |
| 11 | Mar 7-13 | Second PR | 12 | Merged PR #2 | 200 stars âœ… |
| 12 | Mar 14-20 | Third PR | 12 | Merged PRs #3-4 | 300 stars âœ… |
| 13-16 | Mar 21-Apr 17 | Blog #3 | 12/wk | Governance post | 15K views âœ… |
| 17-20 | Apr 18-May 15 | Platform v0.5 | 12/wk | Enterprise features | v0.5 live âœ… |
| 21-24 | May 16-Jun 12 | Conference talk | 12/wk | Talk or YouTube | 500 stars âœ… |
| 25-26 | Jun 13-26 | OSS sprint | 12/wk | 5 more PRs | 10 total PRs âœ… |
| 27-30 | Jun 27-Jul 24 | System design | 10/wk | 30 problems | Mock interviews âœ… |
| 31-34 | Jul 25-Aug 21 | LeetCode | 10/wk | 100 problems | Coding ready âœ… |
| 35-38 | Aug 22-Sep 18 | Behavioral | 10/wk | 15 STAR stories | Stories polished âœ… |
| 39 | Sep 19-25 | Portfolio polish | 12 | Demo video | 700 stars âœ… |
| 40-44 | Sep 26-Oct 30 | Networking | 8/wk | 30 outreach msgs | 10 responses âœ… |
| 45-48 | Oct 31-Nov 27 | Info interviews | 8/wk | 10 calls | Relationships built âœ… |
| 49-52 | Nov 28-Dec 25 | Final push | 8/wk | 1K stars | 100K views âœ… |
| 53 | Dec 26-Jan 1 | Apply Wave 1 | 15 | 5 apps | 3 phone screens âœ… |
| 54-56 | Jan 2-22 | Screens + Wave 2 | 15-20/wk | 10 screens | 5 onsites âœ… |
| 57-59 | Jan 23-Feb 12 | Onsites Wave 1 | 20/wk | 3 onsites | 1 offer âœ… |
| 60-62 | Feb 13-Mar 5 | Wave 3 + Onsites | 20/wk | 4 more onsites | 2 offers âœ… |
| 63-65 | Mar 6-26 | Final onsites | 20/wk | 7 total onsites | 3 offers âœ… |
| 66-68 | Mar 27-Apr 16 | Negotiate | 10/wk | Counter offers | $800K âœ… |
| 69 | Apr 17-23 | ACCEPT | 5 | Sign offer | Offer signed âœ… |
| 70-72 | Apr 24-May 14 | Give notice | 5/wk | Both jobs | Notices given âœ… |
| 73-78 | May 15-Jun 25 | Transition | 56/wk | Hand offs | Clean exits âœ… |
| 78 | Jun 26, 2027 | START | 40 | **FIRST DAY** | **$800K JOB** ðŸŽ‰ |

---

## ðŸŽ¯ CRITICAL SUCCESS FACTORS

**1. Consistency > Intensity**
- 12 hrs/week for 52 weeks = 624 hours
- 20 hrs/week for 20 weeks = 400 hours
- **Consistency wins.** Don't burn out.

**2. Public > Perfect**
- Publish imperfect blog posts
- Push imperfect code
- Ship fast, iterate

**3. Community > Solo**
- Answer questions in MLOps Slack
- Help other vLLM users
- Community remembers helpers

**4. Portfolio > Certifications**
- No more certs needed
- Build, don't study

**5. Network > Cold Apply**
- 70% higher callback rate with referral
- Start networking in Month 10

---

## ðŸ“ˆ EXPECTED OUTCOMES BY MONTH

**Month 3:** 100+ stars, 1 blog post, 1K+ views  
**Month 6:** 500+ stars, 2 blog posts, 30K+ views, 5 OSS PRs  
**Month 9:** 700+ stars, 3 blog posts, 50K+ views, 10 OSS PRs  
**Month 12:** 1K+ stars, 100K+ views, 20 OSS PRs, strong network  
**Month 15:** 2-3 offers, negotiating  
**Month 18:** **$800K role at elite AI company** ðŸš€

---

**START NEXT SATURDAY (WEEK 1).** 
**BY JUNE 2027, YOU COULD BE EARNING $800K/YEAR.**

**The 78-week journey begins with 6 hours this Saturday morning.**

**Will you start?**
